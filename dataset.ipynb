{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Course 4: The `Dataset` Class for Loading Data\n",
    "\n",
    "In this notebook, we are taking `Kaggle`'s some datasets (e.g. `EMNIST`, etc.), to display the usage of `pandas`, `numpy`, etc. and transform them into `torch.utils.data.Dataset` class, then use `DataLoader` to load the data.\n",
    "\n",
    "The `Dataset` class is a class that loads data from a source and returns it in a consistent format. This class is used to provide a consistent way to load data from a variety of sources, such as files, databases, and web services. The `Dataset` class is a subclass of the `torch.utils.data.Dataset` class, which is a class provided by the `PyTorch` library for loading data into a neural network.\n",
    "\n",
    "`DataLoader` is a class that loads data from a `Dataset` object and returns it in batches. The `DataLoader` class is a subclass of the `torch.utils.data.DataLoader` class, which is a class provided by the `PyTorch` library for loading data into a neural network in batches.\n",
    "\n",
    "Above them, the `random_split` function is also a great tool for splitting the dataset into training and validation sets.\n",
    "\n",
    "Actually, there are 3 parts in a hole dataset, including the `Training`, `Evaluation`, and `Testing` parts. The `Training` part is used to train the model, the `Evaluation` part is used to evaluate the model, and the `Testing` part is used to test the model, especially for the unseen data.\n",
    "\n",
    "In this notebook, we will take the `EMNIST` dataset as an example to show how to load the data into the `Dataset` class, and then use the `DataLoader` to load the data in batches."
   ],
   "id": "23c372fd12db3352"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Download the Dataset\n",
    "\n",
    "You can log in [Kaggle](https://www.kaggle.com/) and download the dataset, then upload it to the current directory. Here, we take the [`EMNIST`](https://www.kaggle.com/datasets/crawford/emnist/data) dataset as an example. The size is 1.2 GiB. You can download it and upload it to the current directory. And the article of the dataset is uploaded in [arXiv](https://arxiv.org/pdf/1702.05373v1).\n",
    "\n",
    "We decompress the dataset into `data/EMNIST`, which contains a `.ubyte` file, and a `.csv` file with every row representing an image.\n",
    "\n",
    "Quote from the official website:\n",
    "\n",
    "> Format\n",
    "There are six different splits provided in this dataset and each are provided in two formats:\n",
    "> \n",
    "> 1. Binary (see emnist_source_files.zip)\n",
    "> 2. CSV (combined labels and images)\n",
    ">    - Each row is a separate image\n",
    ">    - 785 columns\n",
    ">    - First column = class_label (see mappings.txt for class label definitions)\n",
    ">    - Each column after represents one pixel value (784 total for a 28 x 28 image)\n",
    "\n"
   ],
   "id": "25f85f239871651a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Through `pandas`, we can load the `CSV` format into a `DataFrame` object. Then we can transform the `DataFrame` object into a `Dataset` object. Finally, we can use the `DataLoader` to load the data in batches.\n",
    "\n",
    "Here, we take the `EMNIST` dataset as an example to show how to load the data into the `Dataset` class, and then use the `DataLoader` to load the data in batches.\n",
    "\n",
    "First, we need to install these packages:"
   ],
   "id": "8ae4bf23f8d2f75d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install pandas numpy",
   "id": "f827cd2ec8c4ed82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Then following what we learned during the IT course, we can easily load the data into the `Dataset`.\n",
    "\n",
    "What should we know: the first column is the label, and the rest columns are the pixels of the image.\n",
    "\n",
    "The first label is in the range of `[1, 26]`, which represents the 26 letters in the alphabet. The first 26 labels are the letters from `A` to `Z`, including the upper case and lower case."
   ],
   "id": "e4b8aa54798e4159"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:47:34.052853Z",
     "start_time": "2024-07-11T09:47:32.073916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device('mps')\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),                        # Convert the tensor to a PIL Image\n",
    "    transforms.RandomRotation(degrees=(-90, -90)),  # Rotate 90 degrees clockwise\n",
    "    transforms.RandomHorizontalFlip(p=1.0),         # Flip left-to-right\n",
    "    transforms.ToTensor()                           # Convert the PIL Image to a tensor\n",
    "])\n",
    "\n",
    "class EMNISTDataset(Dataset):\n",
    "    def __init__(self, target: str):\n",
    "        self.data = pd.read_csv(target)\n",
    "        self.labels = self.data.iloc[:, 0]\n",
    "        self.images = self.data.iloc[:, 1:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images.iloc[idx].values\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        image = torch.tensor(image, dtype=torch.float32).to(device)\n",
    "        label = torch.tensor(label, dtype=torch.long).to(device)\n",
    "        \n",
    "        # Rotate clockwise 90 degrees and horizontal flip\n",
    "        image = image.view(28, 28)\n",
    "        image = transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Load the dataset\n",
    "dataset = EMNISTDataset('data/EMNIST/emnist-letters-train.csv')\n",
    "# Show an example of figure\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "\n",
    "image, label = dataset[randint(0, len(dataset))]\n",
    "image = image.view(28, 28)\n",
    "plt.imshow(image.cpu().numpy(), cmap='gray')\n",
    "plt.title(f'Label: {chr(label + 64)}')\n",
    "plt.show()\n"
   ],
   "id": "8baa1ded167da9a3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Split the Dataset\n",
    "\n",
    "We can split the dataset into 7-3. 70% for training and 30% for validation.\n",
    "\n",
    "We can use the `random_split` function to split the dataset into training and validation sets. The `random_split` function takes the dataset and the lengths of the training and validation sets as arguments and returns the training and validation sets."
   ],
   "id": "c701bf29f1b64c05"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T06:35:34.920603Z",
     "start_time": "2024-07-07T06:35:34.912762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f'Training dataset size: {len(train_dataset)}')\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "79f656547506162b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 62159\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Use ResNet-152 for Training",
   "id": "96c5665d4bf3e22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T06:45:56.401438Z",
     "start_time": "2024-07-07T06:40:45.458026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models.resnet import resnet152\n",
    "from torchvision.models.resnet import ResNet152_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the model\n",
    "model = resnet152(weights = ResNet152_Weights).to(device)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)\n",
    "model.fc = nn.Linear(2048, 26).to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}')"
   ],
   "id": "2daf9ac089111593",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 825/972 [05:09<00:55,  2.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 28\u001B[0m\n\u001B[1;32m     26\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(images)\n\u001B[1;32m     27\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m---> 28\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     29\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     30\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/training/lib/python3.11/site-packages/torch/_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    524\u001B[0m     )\n\u001B[0;32m--> 525\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[1;32m    526\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[1;32m    527\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/training/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m _engine_run_backward(\n\u001B[1;32m    268\u001B[0m     tensors,\n\u001B[1;32m    269\u001B[0m     grad_tensors_,\n\u001B[1;32m    270\u001B[0m     retain_graph,\n\u001B[1;32m    271\u001B[0m     create_graph,\n\u001B[1;32m    272\u001B[0m     inputs,\n\u001B[1;32m    273\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    274\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    275\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/training/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    745\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    746\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b92959e0bfcd3c17"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
