{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Course 4: The `Dataset` Class for Loading Data\n",
    "\n",
    "In this notebook, we are taking `Kaggle`'s some datasets (e.g. `EMNIST`, etc.), to display the usage of `pandas`, `numpy`, etc. and transform them into `torch.utils.data.Dataset` class, then use `DataLoader` to load the data.\n",
    "\n",
    "The `Dataset` class is a class that loads data from a source and returns it in a consistent format. This class is used to provide a consistent way to load data from a variety of sources, such as files, databases, and web services. The `Dataset` class is a subclass of the `torch.utils.data.Dataset` class, which is a class provided by the `PyTorch` library for loading data into a neural network.\n",
    "\n",
    "`DataLoader` is a class that loads data from a `Dataset` object and returns it in batches. The `DataLoader` class is a subclass of the `torch.utils.data.DataLoader` class, which is a class provided by the `PyTorch` library for loading data into a neural network in batches.\n",
    "\n",
    "Above them, the `random_split` function is also a great tool for splitting the dataset into training and validation sets.\n",
    "\n",
    "Actually, there are 3 parts in a hole dataset, including the `Training`, `Evaluation`, and `Testing` parts. The `Training` part is used to train the model, the `Evaluation` part is used to evaluate the model, and the `Testing` part is used to test the model, especially for the unseen data.\n",
    "\n",
    "In this notebook, we will take the `EMNIST` dataset as an example to show how to load the data into the `Dataset` class, and then use the `DataLoader` to load the data in batches."
   ],
   "id": "23c372fd12db3352"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Download the Dataset\n",
    "\n",
    "You can log in [Kaggle](https://www.kaggle.com/) and download the dataset, then upload it to the current directory. Here, we take the [`EMNIST`](https://www.kaggle.com/datasets/crawford/emnist/data) dataset as an example. The size is 1.2 GiB. You can download it and upload it to the current directory. And the article of the dataset is uploaded in [arXiv](https://arxiv.org/pdf/1702.05373v1).\n",
    "\n",
    "We decompress the dataset into `data/EMNIST`, which contains a `.ubyte` file, and a `.csv` file with every row representing an image.\n",
    "\n",
    "Quote from the official website:\n",
    "\n",
    "> Format\n",
    "There are six different splits provided in this dataset and each are provided in two formats:\n",
    "> \n",
    "> 1. Binary (see emnist_source_files.zip)\n",
    "> 2. CSV (combined labels and images)\n",
    ">    - Each row is a separate image\n",
    ">    - 785 columns\n",
    ">    - First column = class_label (see mappings.txt for class label definitions)\n",
    ">    - Each column after represents one pixel value (784 total for a 28 x 28 image)\n",
    "\n"
   ],
   "id": "25f85f239871651a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Through `pandas`, we can load the `CSV` format into a `DataFrame` object. Then we can transform the `DataFrame` object into a `Dataset` object. Finally, we can use the `DataLoader` to load the data in batches.\n",
    "\n",
    "Here, we take the `EMNIST` dataset as an example to show how to load the data into the `Dataset` class, and then use the `DataLoader` to load the data in batches.\n",
    "\n",
    "First, we need to install these packages:"
   ],
   "id": "8ae4bf23f8d2f75d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install pandas numpy",
   "id": "f827cd2ec8c4ed82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Then following what we learned during the IT course, we can easily load the data into the `Dataset`.\n",
    "\n",
    "What should we know: the first column is the label, and the rest columns are the pixels of the image.\n",
    "\n",
    "The first label is in the range of `[1, 26]`, which represents the 26 letters in the alphabet. The first 26 labels are the letters from `A` to `Z`, including the upper case and lower case."
   ],
   "id": "e4b8aa54798e4159"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T06:34:09.954677Z",
     "start_time": "2024-07-07T06:34:08.182966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device('mps')\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),                        # Convert the tensor to a PIL Image\n",
    "    transforms.RandomRotation(degrees=(-90, -90)),  # Rotate 90 degrees clockwise\n",
    "    transforms.RandomHorizontalFlip(p=1.0),         # Flip left-to-right\n",
    "    transforms.ToTensor()                           # Convert the PIL Image to a tensor\n",
    "])\n",
    "\n",
    "class EMNISTDataset(Dataset):\n",
    "    def __init__(self, target: str):\n",
    "        self.data = pd.read_csv(target)\n",
    "        self.labels = self.data.iloc[:, 0]\n",
    "        self.images = self.data.iloc[:, 1:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images.iloc[idx].values\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        image = torch.tensor(image, dtype=torch.float32).to(device)\n",
    "        label = torch.tensor(label, dtype=torch.long).to(device)\n",
    "        \n",
    "        # Rotate clockwise 90 degrees and horizontal flip\n",
    "        image = image.view(28, 28)\n",
    "        image = transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Load the dataset\n",
    "dataset = EMNISTDataset('data/EMNIST/emnist-letters-train.csv')\n",
    "# Show an example of figure\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "\n",
    "image, label = dataset[randint(0, len(dataset))]\n",
    "image = image.view(28, 28)\n",
    "plt.imshow(image.cpu().numpy(), cmap='gray')\n",
    "plt.title(f'Label: {chr(label + 64)}')\n",
    "plt.show()\n"
   ],
   "id": "8baa1ded167da9a3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Split the Dataset\n",
    "\n",
    "We can split the dataset into 7-3. 70% for training and 30% for validation.\n",
    "\n",
    "We can use the `random_split` function to split the dataset into training and validation sets. The `random_split` function takes the dataset and the lengths of the training and validation sets as arguments and returns the training and validation sets."
   ],
   "id": "c701bf29f1b64c05"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T06:35:34.920603Z",
     "start_time": "2024-07-07T06:35:34.912762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f'Training dataset size: {len(train_dataset)}')\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "79f656547506162b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 62159\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Use ResNet-152 for Training",
   "id": "96c5665d4bf3e22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T06:45:56.401438Z",
     "start_time": "2024-07-07T06:40:45.458026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models.resnet import resnet152\n",
    "from torchvision.models.resnet import ResNet152_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the model\n",
    "model = resnet152(weights = ResNet152_Weights).to(device)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)\n",
    "model.fc = nn.Linear(2048, 26).to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}')"
   ],
   "id": "2daf9ac089111593",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 825/972 [05:09<00:55,  2.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 28\u001B[0m\n\u001B[1;32m     26\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(images)\n\u001B[1;32m     27\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m---> 28\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     29\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     30\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/training/lib/python3.11/site-packages/torch/_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    524\u001B[0m     )\n\u001B[0;32m--> 525\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[1;32m    526\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[1;32m    527\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/training/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m _engine_run_backward(\n\u001B[1;32m    268\u001B[0m     tensors,\n\u001B[1;32m    269\u001B[0m     grad_tensors_,\n\u001B[1;32m    270\u001B[0m     retain_graph,\n\u001B[1;32m    271\u001B[0m     create_graph,\n\u001B[1;32m    272\u001B[0m     inputs,\n\u001B[1;32m    273\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    274\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    275\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/training/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    745\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    746\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b92959e0bfcd3c17"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
