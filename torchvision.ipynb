{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Course 2: Introduction to TorchVision\n",
    "\n",
    "`torchvision` is a library that provides a collection of datasets and models for computer vision. It facilitates the process of loading and preprocessing data, and provides a collection of models that can be used for common tasks in computer vision.\n",
    "\n",
    "In this notebook, we'll learn how to use `torchvision` to load and preprocess data, and how to use pre-trained models for tasks like image classification.\n",
    "\n",
    "Then, we will achieve the previous task of classifying images of handwritten digits from the MNIST dataset using a convolutional neural network.\n",
    "\n",
    "We'll also take a look at the CIFAR-10 dataset, which consists of 60000 32x32 px colour images in 10 classes. We'll create a convolutional neural network that can predict the labels of these images with a reasonably high accuracy.\n",
    "\n",
    "Let's start by installing and importing the required libraries."
   ],
   "id": "ee0335b05077f220"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Transform and Datasets\n",
    "\n",
    "`torchvision` provides a module called `transforms` which contains a large number of methods that can be chained together using `transforms.Compose`. Some of the popular transforms are:\n",
    "\n",
    "- `ToTensor`: Converts a numpy array or a PIL image object into a PyTorch tensor.\n",
    "- `Normalize`: Normalizes the input tensor to have a mean and standard deviation of a given value.\n",
    "- `Resize`: Resizes the input PIL image to the given size.\n",
    "- `RandomCrop`: Crops the input PIL image at a random location.\n",
    "- `CenterCrop`: Crops the input PIL image at the center.\n",
    "- `RandomHorizontalFlip`: Randomly flips the input PIL image horizontally.\n",
    "- `RandomVerticalFlip`: Randomly flips the input PIL image vertically.\n",
    "- etc.\n",
    "\n",
    "`torchvision` also provides a module called `datasets` which provides a collection of datasets that can be used to train and test machine learning models. Some of the popular datasets are:\n",
    "\n",
    "- `MNIST`: A dataset of 28x28 px grayscale images of handwritten digits.\n",
    "- `CIFAR10`: A dataset of 32x32 px colour images in 10 classes.\n",
    "- `ImageNet`: A massive dataset of 224x224 px colour images in 1000 classes.\n",
    "- etc.\n",
    "\n",
    "Let's start by importing `torch`, `torchvision`, `torchvision.transforms` and `torchvision.datasets`."
   ],
   "id": "db824c733a9bfab1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T10:13:14.143142Z",
     "start_time": "2024-07-08T10:13:12.520751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the transform to convert the PIL image to a tensor and normalize it\n",
    "example_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "example_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=example_transform)\n",
    "example_loader = DataLoader(example_set, batch_size=4, shuffle=True)\n",
    "\n",
    "# Get a single image and its label from the dataset\n",
    "dataiter = iter(example_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Function to display an image\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(npimg[0], cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# Display the first image in the batch\n",
    "imshow(images[0])\n",
    "print('Label:', labels[0].item())"
   ],
   "id": "b5ad4b7e376fc2cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pre-trained Models\n",
    "\n",
    "`torchvision` provides a collection of pre-trained models that can be used for common tasks in computer vision. Some of the popular models are:\n",
    "\n",
    "- `AlexNet`\n",
    "- `VGG`\n",
    "- `ResNet`\n",
    "- `SqueezeNet`\n",
    "- `DenseNet`\n",
    "- `Inception`\n",
    "- etc.\n",
    "\n",
    "These models have been trained on large datasets like ImageNet and have achieved state-of-the-art performance on tasks like image classification, object detection, and image segmentation.\n",
    "\n",
    "But it is not recommended to use these models during the competition, especially with weights that have been pre-trained on ImageNet. This is because the weights of these models have been optimized for the ImageNet dataset, and may not perform well on other datasets.\n",
    "\n",
    "However, you can use these models as a starting point for your own models. You can load the pre-trained weights, freeze the layers, and fine-tune the model on your own dataset.\n",
    "\n",
    "Let's start by importing `torchvision.models`, and take the `resnet18` model as an example."
   ],
   "id": "1e168e1149b66d6b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T10:15:21.299405Z",
     "start_time": "2024-07-08T10:15:21.127009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Load the pre-trained ResNet-18 model\n",
    "resnet18 = models.resnet18()\n",
    "\n",
    "# Display the model architecture\n",
    "print(resnet18)\n",
    "\n",
    "# Adjust the structure of the net to fit the MNIST dataset\n",
    "resnet18.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "resnet18.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\n",
    "\n",
    "# Display the model architecture with the adjusted final layer\n",
    "print(resnet18)"
   ],
   "id": "72df26806139ff3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MNIST Classification\n",
    "\n",
    "Now, let's use the MNIST dataset to train a convolutional neural network that can classify images of handwritten digits. We'll use the `torchvision` module to load and preprocess the data, and the `torch.nn` module to define the model.\n",
    "\n",
    "We'll start by defining the model architecture, then we'll load the data, train the model, and evaluate its performance.\n",
    "\n",
    "Let's start by defining the model architecture."
   ],
   "id": "6b4eaf68257c6a7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Define the Model\n",
    "\n",
    "We'll define a convolutional neural network with 2 convolutional layers, 2 max pooling layers, and 2 fully connected layers. We'll use the ReLU activation function after each layer, and the softmax function after the final layer to get the class probabilities. It is the easiest way to define a model in PyTorch, but its accuracy is not very high.\n",
    "\n",
    "Let's write some code to define the model architecture."
   ],
   "id": "ec1ab6e36addaf6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T10:17:04.989921Z",
     "start_time": "2024-07-08T10:17:04.666984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assuming you want to use CUDA if available\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc1 = nn.Linear(7*7*64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "2fd5c326875ba642",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have defined a convolutional neural network with 2 convolutional layers, 2 max pooling layers, and 2 fully connected layers. We have also defined the loss function as the cross-entropy loss, and the optimizer as the stochastic gradient descent (SGD) optimizer with a learning rate of 0.001 and a momentum of 0.9. Actually, you can use `Adam`, `RMSprop`, or other optimizers in PyTorch. Now let's load the data and train the model.",
   "id": "f0031790a8286996"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T10:17:36.940731Z",
     "start_time": "2024-07-08T10:17:36.901173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "epochs = 5"
   ],
   "id": "dc4256648957ebe3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Train the Model\n",
    "\n",
    "Recommend: use `tqdm` to show the progress bar.\n",
    "\n",
    "Let's write some code to train the model."
   ],
   "id": "b515181413a65728"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T10:20:27.301129Z",
     "start_time": "2024-07-08T10:20:15.717839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for (images, labels) in tqdm(trainloader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n",
    "        "
   ],
   "id": "ba560c75658b3c57",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:07<00:00, 122.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.12557867169380188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 475/938 [00:03<00:03, 129.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 16\u001B[0m\n\u001B[1;32m     14\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     15\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 16\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m], Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/training/lib/python3.11/site-packages/torch/optim/optimizer.py:391\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    386\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    387\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    388\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    389\u001B[0m             )\n\u001B[0;32m--> 391\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    392\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    394\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/training/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/training/lib/python3.11/site-packages/torch/optim/adam.py:168\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    157\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    159\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    160\u001B[0m         group,\n\u001B[1;32m    161\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    165\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    166\u001B[0m         state_steps)\n\u001B[0;32m--> 168\u001B[0m     adam(\n\u001B[1;32m    169\u001B[0m         params_with_grad,\n\u001B[1;32m    170\u001B[0m         grads,\n\u001B[1;32m    171\u001B[0m         exp_avgs,\n\u001B[1;32m    172\u001B[0m         exp_avg_sqs,\n\u001B[1;32m    173\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    174\u001B[0m         state_steps,\n\u001B[1;32m    175\u001B[0m         amsgrad\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mamsgrad\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    176\u001B[0m         has_complex\u001B[38;5;241m=\u001B[39mhas_complex,\n\u001B[1;32m    177\u001B[0m         beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[1;32m    178\u001B[0m         beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[1;32m    179\u001B[0m         lr\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    180\u001B[0m         weight_decay\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweight_decay\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    181\u001B[0m         eps\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124meps\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    182\u001B[0m         maximize\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    183\u001B[0m         foreach\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mforeach\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    184\u001B[0m         capturable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcapturable\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    185\u001B[0m         differentiable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    186\u001B[0m         fused\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfused\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    187\u001B[0m         grad_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad_scale\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    188\u001B[0m         found_inf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    189\u001B[0m     )\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m/opt/anaconda3/envs/training/lib/python3.11/site-packages/torch/optim/adam.py:318\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    315\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    316\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 318\u001B[0m func(params,\n\u001B[1;32m    319\u001B[0m      grads,\n\u001B[1;32m    320\u001B[0m      exp_avgs,\n\u001B[1;32m    321\u001B[0m      exp_avg_sqs,\n\u001B[1;32m    322\u001B[0m      max_exp_avg_sqs,\n\u001B[1;32m    323\u001B[0m      state_steps,\n\u001B[1;32m    324\u001B[0m      amsgrad\u001B[38;5;241m=\u001B[39mamsgrad,\n\u001B[1;32m    325\u001B[0m      has_complex\u001B[38;5;241m=\u001B[39mhas_complex,\n\u001B[1;32m    326\u001B[0m      beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[1;32m    327\u001B[0m      beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[1;32m    328\u001B[0m      lr\u001B[38;5;241m=\u001B[39mlr,\n\u001B[1;32m    329\u001B[0m      weight_decay\u001B[38;5;241m=\u001B[39mweight_decay,\n\u001B[1;32m    330\u001B[0m      eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[1;32m    331\u001B[0m      maximize\u001B[38;5;241m=\u001B[39mmaximize,\n\u001B[1;32m    332\u001B[0m      capturable\u001B[38;5;241m=\u001B[39mcapturable,\n\u001B[1;32m    333\u001B[0m      differentiable\u001B[38;5;241m=\u001B[39mdifferentiable,\n\u001B[1;32m    334\u001B[0m      grad_scale\u001B[38;5;241m=\u001B[39mgrad_scale,\n\u001B[1;32m    335\u001B[0m      found_inf\u001B[38;5;241m=\u001B[39mfound_inf)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/training/lib/python3.11/site-packages/torch/optim/adam.py:443\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[1;32m    440\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    441\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (exp_avg_sq\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[0;32m--> 443\u001B[0m     param\u001B[38;5;241m.\u001B[39maddcdiv_(exp_avg, denom, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mstep_size)\n\u001B[1;32m    445\u001B[0m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n\u001B[1;32m    446\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m amsgrad \u001B[38;5;129;01mand\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mis_complex(params[i]):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluate the Model\n",
    "\n",
    "Let's write some code to evaluate the model. You should clean the gradients and set the model to evaluation mode before evaluating the model."
   ],
   "id": "11d9f86b3c6102d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T10:18:49.810584Z",
     "start_time": "2024-07-08T10:18:48.270609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with (torch.no_grad()):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(testloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the 10000 test images: {100 * correct / total} %')"
   ],
   "id": "b28962d47c5e303e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:01<00:00, 102.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 99.09 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Show one prediction with image",
   "id": "38385b1275975867"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T10:19:02.872796Z",
     "start_time": "2024-07-08T10:19:02.727296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get a random image from the test set\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Move the image to the device\n",
    "images = images.to(device)\n",
    "\n",
    "# Get the prediction\n",
    "outputs = model(images)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "# Display the image and the prediction\n",
    "imshow(images[0].cpu())\n",
    "print('Label:', labels[0].item())"
   ],
   "id": "4e166d5aa0a0bf34",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAaqElEQVR4nO3df2xV9f3H8VeL9ILaXiylvb2jQEEFwy8ng9rwYygNtC4GtEtA/QMWAoFdzLDzx7qIKFvSjSWOuCD+s8BMxF+JQCRLMym2hNliqDDCph3tugGBFsVxbylSGP18/yDer1cKeMq9ffdeno/kJPTe8+l9ezzhyWlvT9Occ04AAPSxdOsBAAA3JwIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBM3GI9wLd1d3frxIkTyszMVFpamvU4AACPnHPq6OhQMBhUevrVr3P6XYBOnDihgoIC6zEAADfo2LFjGj58+FWf73dfgsvMzLQeAQAQB9f7+zxhAdq4caNGjRqlQYMGqaioSB9//PF3WseX3QAgNVzv7/OEBOjtt99WRUWF1q5dq08++USTJ0/WvHnzdOrUqUS8HAAgGbkEmDZtmguFQtGPL1265ILBoKuqqrru2nA47CSxsbGxsSX5Fg6Hr/n3fdyvgC5cuKDGxkaVlJREH0tPT1dJSYnq6+uv2L+rq0uRSCRmAwCkvrgH6IsvvtClS5eUl5cX83heXp7a2tqu2L+qqkp+vz+68Q44ALg5mL8LrrKyUuFwOLodO3bMeiQAQB+I+88B5eTkaMCAAWpvb495vL29XYFA4Ir9fT6ffD5fvMcAAPRzcb8CysjI0JQpU1RTUxN9rLu7WzU1NSouLo73ywEAklRC7oRQUVGhxYsX6wc/+IGmTZumDRs2qLOzUz/5yU8S8XIAgCSUkAAtXLhQn3/+uV544QW1tbXp3nvvVXV19RVvTAAA3LzSnHPOeohvikQi8vv91mMAAG5QOBxWVlbWVZ83fxccAODmRIAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATMQ9QC+++KLS0tJitnHjxsX7ZQAASe6WRHzS8ePHa9euXf//Irck5GUAAEksIWW45ZZbFAgEEvGpAQApIiHfAzpy5IiCwaBGjx6tJ554QkePHr3qvl1dXYpEIjEbACD1xT1ARUVF2rJli6qrq7Vp0ya1trZq5syZ6ujo6HH/qqoq+f3+6FZQUBDvkQAA/VCac84l8gXOnDmjkSNH6uWXX9bSpUuveL6rq0tdXV3RjyORCBECgBQQDoeVlZV11ecT/u6AIUOG6O6771Zzc3OPz/t8Pvl8vkSPAQDoZxL+c0Bnz55VS0uL8vPzE/1SAIAkEvcAPf3006qrq9O///1vffTRR3rkkUc0YMAAPfbYY/F+KQBAEov7l+COHz+uxx57TKdPn9awYcM0Y8YMNTQ0aNiwYfF+KQBAEkv4mxC8ikQi8vv91mMAAG7Q9d6EwL3gAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATCf+FdOhbP/7xjz2vWbZsWa9e68SJE57XnD9/3vOaN954w/OatrY2z2skXfUXJwKIP66AAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYCLNOeesh/imSCQiv99vPUbS+te//uV5zahRo+I/iLGOjo5erfv73/8e50kQb8ePH/e8Zv369b16rf379/dqHS4Lh8PKysq66vNcAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJm6xHgDxtWzZMs9rJk2a1KvX+vTTTz2vueeeezyvue+++zyvmT17tuc1knT//fd7XnPs2DHPawoKCjyv6Uv/+9//PK/5/PPPPa/Jz8/3vKY3jh492qt13Iw0sbgCAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMcDPSFFNTU9Mna3qrurq6T17njjvu6NW6e++91/OaxsZGz2umTp3qeU1fOn/+vOc1//znPz2v6c0NbbOzsz2vaWlp8bwGiccVEADABAECAJjwHKA9e/bo4YcfVjAYVFpamrZv3x7zvHNOL7zwgvLz8zV48GCVlJToyJEj8ZoXAJAiPAeos7NTkydP1saNG3t8fv369XrllVf02muvad++fbrttts0b968Xn1NGQCQujy/CaGsrExlZWU9Puec04YNG/T8889r/vz5kqTXX39deXl52r59uxYtWnRj0wIAUkZcvwfU2tqqtrY2lZSURB/z+/0qKipSfX19j2u6uroUiURiNgBA6otrgNra2iRJeXl5MY/n5eVFn/u2qqoq+f3+6FZQUBDPkQAA/ZT5u+AqKysVDoej27Fjx6xHAgD0gbgGKBAISJLa29tjHm9vb48+920+n09ZWVkxGwAg9cU1QIWFhQoEAjE/WR+JRLRv3z4VFxfH86UAAEnO87vgzp49q+bm5ujHra2tOnjwoLKzszVixAitXr1av/71r3XXXXepsLBQa9asUTAY1IIFC+I5NwAgyXkO0P79+/XAAw9EP66oqJAkLV68WFu2bNGzzz6rzs5OLV++XGfOnNGMGTNUXV2tQYMGxW9qAEDSS3POOeshvikSicjv91uPAcCj8vJyz2veeecdz2sOHz7sec03/9HsxZdfftmrdbgsHA5f8/v65u+CAwDcnAgQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGDC869jAJD6cnNzPa959dVXPa9JT/f+b+B169Z5XsNdrfsnroAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABPcjBTAFUKhkOc1w4YN87zmv//9r+c1TU1Nntegf+IKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwc1IgRQ2ffr0Xq37xS9+EedJerZgwQLPaw4fPhz/QWCCKyAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQ3IwVS2EMPPdSrdQMHDvS8pqamxvOa+vp6z2uQOrgCAgCYIEAAABOeA7Rnzx49/PDDCgaDSktL0/bt22OeX7JkidLS0mK20tLSeM0LAEgRngPU2dmpyZMna+PGjVfdp7S0VCdPnoxub7755g0NCQBIPZ7fhFBWVqaysrJr7uPz+RQIBHo9FAAg9SXke0C1tbXKzc3V2LFjtXLlSp0+ffqq+3Z1dSkSicRsAIDUF/cAlZaW6vXXX1dNTY1++9vfqq6uTmVlZbp06VKP+1dVVcnv90e3goKCeI8EAOiH4v5zQIsWLYr+eeLEiZo0aZLGjBmj2tpazZkz54r9KysrVVFREf04EokQIQC4CST8bdijR49WTk6Ompube3ze5/MpKysrZgMApL6EB+j48eM6ffq08vPzE/1SAIAk4vlLcGfPno25mmltbdXBgweVnZ2t7OxsvfTSSyovL1cgEFBLS4ueffZZ3XnnnZo3b15cBwcAJDfPAdq/f78eeOCB6Mdff/9m8eLF2rRpkw4dOqQ//elPOnPmjILBoObOnatf/epX8vl88ZsaAJD00pxzznqIb4pEIvL7/dZjAP3O4MGDPa/Zu3dvr15r/Pjxntc8+OCDntd89NFHntcgeYTD4Wt+X597wQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMBE3H8lN4DEeOaZZzyv+f73v9+r16qurva8hjtbwyuugAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE9yMFDDwox/9yPOaNWvWeF4TiUQ8r5GkdevW9Wod4AVXQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACW5GCtygoUOHel7zyiuveF4zYMAAz2v+/Oc/e14jSQ0NDb1aB3jBFRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIKbkQLf0JsbflZXV3teU1hY6HlNS0uL5zVr1qzxvAboK1wBAQBMECAAgAlPAaqqqtLUqVOVmZmp3NxcLViwQE1NTTH7nD9/XqFQSEOHDtXtt9+u8vJytbe3x3VoAEDy8xSguro6hUIhNTQ06IMPPtDFixc1d+5cdXZ2Rvd56qmn9P777+vdd99VXV2dTpw4oUcffTTugwMAkpunNyF8+5utW7ZsUW5urhobGzVr1iyFw2H98Y9/1NatW/Xggw9KkjZv3qx77rlHDQ0Nuv/+++M3OQAgqd3Q94DC4bAkKTs7W5LU2NioixcvqqSkJLrPuHHjNGLECNXX1/f4Obq6uhSJRGI2AEDq63WAuru7tXr1ak2fPl0TJkyQJLW1tSkjI0NDhgyJ2TcvL09tbW09fp6qqir5/f7oVlBQ0NuRAABJpNcBCoVCOnz4sN56660bGqCyslLhcDi6HTt27IY+HwAgOfTqB1FXrVqlnTt3as+ePRo+fHj08UAgoAsXLujMmTMxV0Ht7e0KBAI9fi6fzyefz9ebMQAASczTFZBzTqtWrdK2bdu0e/fuK36ae8qUKRo4cKBqamqijzU1Neno0aMqLi6Oz8QAgJTg6QooFApp69at2rFjhzIzM6Pf1/H7/Ro8eLD8fr+WLl2qiooKZWdnKysrS08++aSKi4t5BxwAIIanAG3atEmSNHv27JjHN2/erCVLlkiSfv/73ys9PV3l5eXq6urSvHnz9Oqrr8ZlWABA6khzzjnrIb4pEonI7/dbj4Gb1N133+15zWeffZaASa40f/58z2vef//9BEwCfDfhcFhZWVlXfZ57wQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMBEr34jKtDfjRw5slfr/vKXv8R5kp4988wzntfs3LkzAZMAdrgCAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMcDNSpKTly5f3at2IESPiPEnP6urqPK9xziVgEsAOV0AAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAluRop+b8aMGZ7XPPnkkwmYBEA8cQUEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJjgZqTo92bOnOl5ze23356ASXrW0tLiec3Zs2cTMAmQXLgCAgCYIEAAABOeAlRVVaWpU6cqMzNTubm5WrBggZqammL2mT17ttLS0mK2FStWxHVoAEDy8xSguro6hUIhNTQ06IMPPtDFixc1d+5cdXZ2xuy3bNkynTx5MrqtX78+rkMDAJKfpzchVFdXx3y8ZcsW5ebmqrGxUbNmzYo+fuuttyoQCMRnQgBASrqh7wGFw2FJUnZ2dszjb7zxhnJycjRhwgRVVlbq3LlzV/0cXV1dikQiMRsAIPX1+m3Y3d3dWr16taZPn64JEyZEH3/88cc1cuRIBYNBHTp0SM8995yampr03nvv9fh5qqqq9NJLL/V2DABAkup1gEKhkA4fPqy9e/fGPL58+fLonydOnKj8/HzNmTNHLS0tGjNmzBWfp7KyUhUVFdGPI5GICgoKejsWACBJ9CpAq1at0s6dO7Vnzx4NHz78mvsWFRVJkpqbm3sMkM/nk8/n680YAIAk5ilAzjk9+eST2rZtm2pra1VYWHjdNQcPHpQk5efn92pAAEBq8hSgUCikrVu3aseOHcrMzFRbW5skye/3a/DgwWppadHWrVv10EMPaejQoTp06JCeeuopzZo1S5MmTUrIfwAAIDl5CtCmTZskXf5h02/avHmzlixZooyMDO3atUsbNmxQZ2enCgoKVF5erueffz5uAwMAUoPnL8FdS0FBgerq6m5oIADAzYG7YQPf8Le//c3zmjlz5nhe8+WXX3peA6QabkYKADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJhIc9e7xXUfi0Qi8vv91mMAAG5QOBxWVlbWVZ/nCggAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAICJfhegfnZrOgBAL13v7/N+F6COjg7rEQAAcXC9v8/73d2wu7u7deLECWVmZiotLS3muUgkooKCAh07duyad1hNdRyHyzgOl3EcLuM4XNYfjoNzTh0dHQoGg0pPv/p1zi19ONN3kp6eruHDh19zn6ysrJv6BPsax+EyjsNlHIfLOA6XWR+H7/Jrdfrdl+AAADcHAgQAMJFUAfL5fFq7dq18Pp/1KKY4DpdxHC7jOFzGcbgsmY5Dv3sTAgDg5pBUV0AAgNRBgAAAJggQAMAEAQIAmEiaAG3cuFGjRo3SoEGDVFRUpI8//th6pD734osvKi0tLWYbN26c9VgJt2fPHj388MMKBoNKS0vT9u3bY553zumFF15Qfn6+Bg8erJKSEh05csRm2AS63nFYsmTJFedHaWmpzbAJUlVVpalTpyozM1O5ublasGCBmpqaYvY5f/68QqGQhg4dqttvv13l5eVqb283mjgxvstxmD179hXnw4oVK4wm7llSBOjtt99WRUWF1q5dq08++USTJ0/WvHnzdOrUKevR+tz48eN18uTJ6LZ3717rkRKus7NTkydP1saNG3t8fv369XrllVf02muvad++fbrttts0b948nT9/vo8nTazrHQdJKi0tjTk/3nzzzT6cMPHq6uoUCoXU0NCgDz74QBcvXtTcuXPV2dkZ3eepp57S+++/r3fffVd1dXU6ceKEHn30UcOp4++7HAdJWrZsWcz5sH79eqOJr8IlgWnTprlQKBT9+NKlSy4YDLqqqirDqfre2rVr3eTJk63HMCXJbdu2Lfpxd3e3CwQC7ne/+130sTNnzjifz+fefPNNgwn7xrePg3POLV682M2fP99kHiunTp1yklxdXZ1z7vL/+4EDB7p33303us+nn37qJLn6+nqrMRPu28fBOed++MMfup/97Gd2Q30H/f4K6MKFC2psbFRJSUn0sfT0dJWUlKi+vt5wMhtHjhxRMBjU6NGj9cQTT+jo0aPWI5lqbW1VW1tbzPnh9/tVVFR0U54ftbW1ys3N1dixY7Vy5UqdPn3aeqSECofDkqTs7GxJUmNjoy5evBhzPowbN04jRoxI6fPh28fha2+88YZycnI0YcIEVVZW6ty5cxbjXVW/uxnpt33xxRe6dOmS8vLyYh7Py8vTZ599ZjSVjaKiIm3ZskVjx47VyZMn9dJLL2nmzJk6fPiwMjMzrccz0dbWJkk9nh9fP3ezKC0t1aOPPqrCwkK1tLTol7/8pcrKylRfX68BAwZYjxd33d3dWr16taZPn64JEyZIunw+ZGRkaMiQITH7pvL50NNxkKTHH39cI0eOVDAY1KFDh/Tcc8+pqalJ7733nuG0sfp9gPD/ysrKon+eNGmSioqKNHLkSL3zzjtaunSp4WToDxYtWhT988SJEzVp0iSNGTNGtbW1mjNnjuFkiREKhXT48OGb4vug13K147B8+fLonydOnKj8/HzNmTNHLS0tGjNmTF+P2aN+/yW4nJwcDRgw4Ip3sbS3tysQCBhN1T8MGTJEd999t5qbm61HMfP1OcD5caXRo0crJycnJc+PVatWaefOnfrwww9jfn1LIBDQhQsXdObMmZj9U/V8uNpx6ElRUZEk9avzod8HKCMjQ1OmTFFNTU30se7ubtXU1Ki4uNhwMntnz55VS0uL8vPzrUcxU1hYqEAgEHN+RCIR7du376Y/P44fP67Tp0+n1PnhnNOqVau0bds27d69W4WFhTHPT5kyRQMHDow5H5qamnT06NGUOh+udxx6cvDgQUnqX+eD9bsgvou33nrL+Xw+t2XLFvePf/zDLV++3A0ZMsS1tbVZj9anfv7zn7va2lrX2trq/vrXv7qSkhKXk5PjTp06ZT1aQnV0dLgDBw64AwcOOEnu5ZdfdgcOHHD/+c9/nHPO/eY3v3FDhgxxO3bscIcOHXLz5893hYWF7quvvjKePL6udRw6Ojrc008/7err611ra6vbtWuXu++++9xdd93lzp8/bz163KxcudL5/X5XW1vrTp48Gd3OnTsX3WfFihVuxIgRbvfu3W7//v2uuLjYFRcXG04df9c7Ds3NzW7dunVu//79rrW11e3YscONHj3azZo1y3jyWEkRIOec+8Mf/uBGjBjhMjIy3LRp01xDQ4P1SH1u4cKFLj8/32VkZLjvfe97buHCha65udl6rIT78MMPnaQrtsWLFzvnLr8Ve82aNS4vL8/5fD43Z84c19TUZDt0AlzrOJw7d87NnTvXDRs2zA0cONCNHDnSLVu2LOX+kdbTf78kt3nz5ug+X331lfvpT3/q7rjjDnfrrbe6Rx55xJ08edJu6AS43nE4evSomzVrlsvOznY+n8/deeed7plnnnHhcNh28G/h1zEAAEz0++8BAQBSEwECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABg4v8AjVqFRqQZEfIAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Save the Model\n",
    "\n",
    "You can either save the entire model or just the model parameters. It is recommended to save the model parameters, as it is more space-efficient and allows you to load the model architecture separately."
   ],
   "id": "91b5e6281e4b2dbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T04:52:34.187146Z",
     "start_time": "2024-07-07T04:52:34.122538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the state dict\n",
    "torch.save(model.state_dict(), './data/state-dict.pth')\n",
    "\n",
    "# Load the entire model\n",
    "torch.save(model, './data/model.pth')"
   ],
   "id": "c2f4baee226a46a7",
   "outputs": [],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
