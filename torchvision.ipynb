{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Course 2: Introduction to TorchVision\n",
    "\n",
    "`torchvision` is a library that provides a collection of datasets and models for computer vision. It facilitates the process of loading and preprocessing data, and provides a collection of models that can be used for common tasks in computer vision.\n",
    "\n",
    "In this notebook, we'll learn how to use `torchvision` to load and preprocess data, and how to use pre-trained models for tasks like image classification.\n",
    "\n",
    "Then, we will achieve the previous task of classifying images of handwritten digits from the MNIST dataset using a convolutional neural network.\n",
    "\n",
    "We'll also take a look at the CIFAR-10 dataset, which consists of 60000 32x32 px colour images in 10 classes. We'll create a convolutional neural network that can predict the labels of these images with a reasonably high accuracy.\n",
    "\n",
    "Let's start by installing and importing the required libraries."
   ],
   "id": "ee0335b05077f220"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Transform and Datasets\n",
    "\n",
    "`torchvision` provides a module called `transforms` which contains a large number of methods that can be chained together using `transforms.Compose`. Some of the popular transforms are:\n",
    "\n",
    "- `ToTensor`: Converts a numpy array or a PIL image object into a PyTorch tensor.\n",
    "- `Normalize`: Normalizes the input tensor to have a mean and standard deviation of a given value.\n",
    "- `Resize`: Resizes the input PIL image to the given size.\n",
    "- `RandomCrop`: Crops the input PIL image at a random location.\n",
    "- `CenterCrop`: Crops the input PIL image at the center.\n",
    "- `RandomHorizontalFlip`: Randomly flips the input PIL image horizontally.\n",
    "- `RandomVerticalFlip`: Randomly flips the input PIL image vertically.\n",
    "- etc.\n",
    "\n",
    "`torchvision` also provides a module called `datasets` which provides a collection of datasets that can be used to train and test machine learning models. Some of the popular datasets are:\n",
    "\n",
    "- `MNIST`: A dataset of 28x28 px grayscale images of handwritten digits.\n",
    "- `CIFAR10`: A dataset of 32x32 px colour images in 10 classes.\n",
    "- `ImageNet`: A massive dataset of 224x224 px colour images in 1000 classes.\n",
    "- etc.\n",
    "\n",
    "Let's start by importing `torch`, `torchvision`, `torchvision.transforms` and `torchvision.datasets`."
   ],
   "id": "db824c733a9bfab1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T04:48:01.090716Z",
     "start_time": "2024-07-07T04:48:00.985222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the transform to convert the PIL image to a tensor and normalize it\n",
    "example_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "example_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=example_transform)\n",
    "example_loader = DataLoader(example_set, batch_size=4, shuffle=True)\n",
    "\n",
    "# Get a single image and its label from the dataset\n",
    "dataiter = iter(example_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Function to display an image\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(npimg[0], cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# Display the first image in the batch\n",
    "imshow(images[0])\n",
    "print('Label:', labels[0].item())"
   ],
   "id": "b5ad4b7e376fc2cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pre-trained Models\n",
    "\n",
    "`torchvision` provides a collection of pre-trained models that can be used for common tasks in computer vision. Some of the popular models are:\n",
    "\n",
    "- `AlexNet`\n",
    "- `VGG`\n",
    "- `ResNet`\n",
    "- `SqueezeNet`\n",
    "- `DenseNet`\n",
    "- `Inception`\n",
    "- etc.\n",
    "\n",
    "These models have been trained on large datasets like ImageNet and have achieved state-of-the-art performance on tasks like image classification, object detection, and image segmentation.\n",
    "\n",
    "But it is not recommended to use these models during the competition, especially with weights that have been pre-trained on ImageNet. This is because the weights of these models have been optimized for the ImageNet dataset, and may not perform well on other datasets.\n",
    "\n",
    "However, you can use these models as a starting point for your own models. You can load the pre-trained weights, freeze the layers, and fine-tune the model on your own dataset.\n",
    "\n",
    "Let's start by importing `torchvision.models`, and take the `resnet18` model as an example."
   ],
   "id": "1e168e1149b66d6b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T04:48:01.308125Z",
     "start_time": "2024-07-07T04:48:01.108442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Load the pre-trained ResNet-18 model\n",
    "resnet18 = models.resnet18()\n",
    "\n",
    "# Display the model architecture\n",
    "print(resnet18)\n",
    "\n",
    "# Adjust the structure of the net to fit the MNIST dataset\n",
    "resnet18.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "resnet18.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\n",
    "\n",
    "# Display the model architecture with the adjusted final layer\n",
    "print(resnet18)"
   ],
   "id": "72df26806139ff3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MNIST Classification\n",
    "\n",
    "Now, let's use the MNIST dataset to train a convolutional neural network that can classify images of handwritten digits. We'll use the `torchvision` module to load and preprocess the data, and the `torch.nn` module to define the model.\n",
    "\n",
    "We'll start by defining the model architecture, then we'll load the data, train the model, and evaluate its performance.\n",
    "\n",
    "Let's start by defining the model architecture."
   ],
   "id": "6b4eaf68257c6a7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Define the Model\n",
    "\n",
    "We'll define a convolutional neural network with 2 convolutional layers, 2 max pooling layers, and 2 fully connected layers. We'll use the ReLU activation function after each layer, and the softmax function after the final layer to get the class probabilities. It is the easiest way to define a model in PyTorch, but its accuracy is not very high.\n",
    "\n",
    "Let's write some code to define the model architecture."
   ],
   "id": "ec1ab6e36addaf6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T04:48:01.408113Z",
     "start_time": "2024-07-07T04:48:01.308958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assuming you want to use CUDA if available\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc1 = nn.Linear(7*7*64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "2fd5c326875ba642",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have defined a convolutional neural network with 2 convolutional layers, 2 max pooling layers, and 2 fully connected layers. We have also defined the loss function as the cross-entropy loss, and the optimizer as the stochastic gradient descent (SGD) optimizer with a learning rate of 0.001 and a momentum of 0.9. Actually, you can use `Adam`, `RMSprop`, or other optimizers in PyTorch. Now let's load the data and train the model.",
   "id": "f0031790a8286996"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T04:48:01.428580Z",
     "start_time": "2024-07-07T04:48:01.409153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "epochs = 5"
   ],
   "id": "dc4256648957ebe3",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Train the Model\n",
    "\n",
    "Recommend: use `tqdm` to show the progress bar.\n",
    "\n",
    "Let's write some code to train the model."
   ],
   "id": "b515181413a65728"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T04:49:47.781003Z",
     "start_time": "2024-07-07T04:49:14.808123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for (images, labels) in tqdm(trainloader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n",
    "        "
   ],
   "id": "ba560c75658b3c57",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:06<00:00, 143.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.00042134529212489724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:06<00:00, 142.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 0.1186581701040268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:06<00:00, 141.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 0.0005305582890287042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:06<00:00, 142.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 0.026803994551301003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:06<00:00, 141.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 7.023017678875476e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluate the Model\n",
    "\n",
    "Let's write some code to evaluate the model. You should clean the gradients and set the model to evaluation mode before evaluating the model."
   ],
   "id": "11d9f86b3c6102d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T04:49:58.421126Z",
     "start_time": "2024-07-07T04:49:57.217728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with (torch.no_grad()):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(testloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the 10000 test images: {100 * correct / total} %')"
   ],
   "id": "b28962d47c5e303e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:01<00:00, 130.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 98.77 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Show one prediction with image",
   "id": "38385b1275975867"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T04:51:22.518262Z",
     "start_time": "2024-07-07T04:51:22.443306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get a random image from the test set\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Move the image to the device\n",
    "images = images.to(device)\n",
    "\n",
    "# Get the prediction\n",
    "outputs = model(images)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "# Display the image and the prediction\n",
    "imshow(images[0].cpu())\n",
    "print('Label:', labels[0].item())"
   ],
   "id": "4e166d5aa0a0bf34",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Save the Model\n",
    "\n",
    "You can either save the entire model or just the model parameters. It is recommended to save the model parameters, as it is more space-efficient and allows you to load the model architecture separately."
   ],
   "id": "91b5e6281e4b2dbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T04:52:34.187146Z",
     "start_time": "2024-07-07T04:52:34.122538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the state dict\n",
    "torch.save(model.state_dict(), './data/state-dict.pth')\n",
    "\n",
    "# Load the entire model\n",
    "torch.save(model, './data/model.pth')"
   ],
   "id": "c2f4baee226a46a7",
   "outputs": [],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
