{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Course 3: Embedded Layer for Natural Language Processing\n",
    "\n",
    "In this notebook, we will use `PyTorch` to get started with natural language processing. I recommend you to use `spaCy` to do the tokenization and the preprocessing of the text. It is more powerful than the `nltk` library."
   ],
   "id": "8016e6566ef10024"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Install the libraries\n",
    "\n",
    "The `spaCy` library is not installed by default. You can install it by running the following command:"
   ],
   "id": "165ecc5167420199"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T08:46:29.295548Z",
     "start_time": "2024-07-07T08:46:29.293550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if using macOS\n",
    "!pip install 'spacy[transformers,lookups,apple]' torchtext transformers\n",
    "# if using Windows with GPU\n",
    "# !pip install -U 'spacy[cuda11x,transformers,lookups]' torchtext transformers\n",
    "# if using Windows without GPU\n",
    "# !pip install -U 'spacy[transformers,lookups]' torchtext transformers\n",
    "# !pip install spacy torchtext transformers\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download zh_core_web_sm"
   ],
   "id": "57487e9053db0b0a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Because the competition contains Chinese text, we will use the `zh_core_web_sm` model to tokenize the Chinese text. The `en_core_web_sm` model is used to tokenize the English text.\n",
    "\n",
    "The `torchtext` library is used to preprocess the text and create the dataloader. It is a powerful library that can help you to preprocess the text and create the dataloader in a few lines of code.\n",
    "\n",
    "The `transformers` library is used to load the pre-trained model, provided by the Hugging Face team. The pre-trained model is used to convert the text into a vector representation. The vector representation can be used as input to the neural network.\n",
    "\n",
    "However, it is not recommended to use pre-trained model during the competition. But it is a good practice to use it in the real world project.\n",
    "\n",
    "Let's start by importing the libraries."
   ],
   "id": "ffa7dea61c84c876"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## A small introduction to spaCy\n",
    "\n",
    "`spaCy` is a powerful library that can help you to tokenize the text. It is more powerful than the `nltk` library. You can vectorize the text and preprocess the text using the `spaCy` library.\n",
    "\n",
    "To get started, you need to load models, which is downloaded during the installation session. You can load the models by running the following code:"
   ],
   "id": "3dbbe8f6cce23939"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T02:22:44.601406Z",
     "start_time": "2024-07-09T02:22:43.071425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "id": "4291bbd0dc7f8c3e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/training/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/anaconda3/envs/training/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then, you can tokenize the text (e.g. \"I am learning Machine Learning & Natural Language Processing\") by running the following code:",
   "id": "8b2ab1e696341f6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T02:25:39.293252Z",
     "start_time": "2024-07-09T02:25:39.258710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "doc = nlp(\"I am learning Machine Learning & Natural Language Processing.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ],
   "id": "290a3a7548361882",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON nsubj\n",
      "am AUX aux\n",
      "learning VERB ROOT\n",
      "Machine PROPN compound\n",
      "Learning PROPN dobj\n",
      "& CCONJ cc\n",
      "Natural PROPN compound\n",
      "Language PROPN compound\n",
      "Processing PROPN conj\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As you can see, the `spaCy` library can figure out every part of the text, such as the token, part of speech, and dependency. It is a powerful library that can help you to preprocess the text and create the dataloader.",
   "id": "c344f9989845bca9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Using Chinese\n",
    "\n",
    "There are numerous differences between English-typical languages and Chinese-typical languages. Chinese doesn't need spaces to split words, but it is harder than English to split the word in Machine Learning. But, don't worry, spaCy helps us to tokenize the Chinese text. You can load the Chinese model by running the following code:"
   ],
   "id": "48a26df152b6ceb6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T02:26:56.460534Z",
     "start_time": "2024-07-09T02:26:55.789098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nlp_chinese = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "doc = nlp_chinese(\"我正在学习机器学习和自然语言处理。\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n",
    "\n",
    "nanjing_bridge = nlp_chinese(\"南京市长江大桥是中国南京的一座桥梁。\")\n",
    "\n",
    "for token in nanjing_bridge:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ],
   "id": "df8a5c77909ca040",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 PRON nsubj\n",
      "正在 ADV advmod\n",
      "学习 VERB ROOT\n",
      "机器 NOUN dobj\n",
      "学习 VERB conj\n",
      "和 CCONJ cc\n",
      "自然 NOUN compound:nn\n",
      "语言 NOUN compound:nn\n",
      "处理 NOUN conj\n",
      "。 PUNCT punct\n",
      "南京市 PROPN compound:nn\n",
      "长江 PROPN compound:nn\n",
      "大桥 NOUN nsubj\n",
      "是 VERB cop\n",
      "中国 PROPN name\n",
      "南京 PROPN nmod:assmod\n",
      "的 PART case\n",
      "一 NUM nummod\n",
      "座 NUM mark:clf\n",
      "桥梁 NOUN ROOT\n",
      "。 PUNCT punct\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T02:42:38.917734Z",
     "start_time": "2024-07-09T02:42:10.382514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torchtext import data, datasets\n",
    "import spacy\n",
    "\n",
    "# Load spaCy models\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_zh = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "# Define the fields\n",
    "TEXT = data.Field(tokenize=lambda x: [tok.text for tok in nlp_en.tokenizer(x)], lower=True)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "# Load IMDb dataset\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train_data, valid_data = train_data.split()"
   ],
   "id": "39eea7c1902c15ce",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/training/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/anaconda3/envs/training/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The `TEXT` field is used to tokenize the text. The `LABEL` field is used to store the label. The `datasets.IMDB.splits` function is used to load the IMDb dataset. The `train_data` and `test_data` are used to store the training and testing data. The `train_data` is further split into `train_data` and `valid_data`.",
   "id": "ba4c7c136f8eaff7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Named entity recognition\n",
    "\n",
    "`spaCy` can also be used to recognize the named entity in the text. The named entity is the real-world object that is assigned a name. For example, the named entity in the text \"Apple is a company\" is \"Apple\". You can recognize the named entity by running the following code:"
   ],
   "id": "3180b280b5247b72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T02:31:34.976984Z",
     "start_time": "2024-07-09T02:31:34.942743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "doc = nlp(\"Apple is a company, and apple is a fruit. By the way, Apple is located in California.\\n Tiananmen Square is located in Beijing, China.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "doc = nlp(\"Zhenhai High School is a high school located in Ningbo, Zhejiang, China. Students in a classroom, learning NLP via Kaggle & GitHub in this class. It cost me 40 minutes, but it is paid for $0.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "doc = nlp_chinese(\"南京市长江大桥是中国南京的一座桥梁。\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ],
   "id": "7326b3de83bf4ddc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Apple ORG\n",
      "California GPE\n",
      "Tiananmen Square FAC\n",
      "Beijing GPE\n",
      "China GPE\n",
      "Zhenhai High School ORG\n",
      "Ningbo GPE\n",
      "Zhejiang GPE\n",
      "China GPE\n",
      "NLP ORG\n",
      "Kaggle & GitHub ORG\n",
      "40 minutes TIME\n",
      "0 MONEY\n",
      "南京市 GPE\n",
      "长江大桥 FAC\n",
      "中国 GPE\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here, the `ent.text` is the named entity, and the `ent.label_` is the type of the named entity. The named entity can be a person, organization, location, product, event, and so on. The named entity recognition is a powerful tool that can help you to extract the information from the text.",
   "id": "11648c041801cecd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Transplant to Chinese Custom Dataset",
   "id": "bfe4ab9b10d3c7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T02:42:46.772969Z",
     "start_time": "2024-07-09T02:42:46.389643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Custom dataset for Chinese (example)\n",
    "chinese_sentences = [\n",
    "    (\"我今天很开心\", \"pos\"),\n",
    "    (\"这真是糟糕透了\", \"neg\"),\n",
    "    (\"天气不错\", \"pos\"),\n",
    "    (\"我感到很悲伤\", \"neg\")\n",
    "]\n",
    "\n",
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self, examples, fields, **kwargs):\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, fields, train_examples=None, val_examples=None, test_examples=None, **kwargs):\n",
    "        return (cls(train_examples, fields, **kwargs),\n",
    "                cls(val_examples, fields, **kwargs),\n",
    "                cls(test_examples, fields, **kwargs))\n",
    "\n",
    "# Create examples for the custom dataset\n",
    "chinese_examples = [data.Example.fromlist([sentence, label], [('text', TEXT), ('label', LABEL)]) for sentence, label in chinese_sentences]\n",
    "chinese_dataset = CustomDataset(chinese_examples, [('text', TEXT), ('label', LABEL)])\n",
    "\n",
    "# Build the vocabulary\n",
    "TEXT.build_vocab(train_data, max_size=25000)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# Add Chinese tokens to the vocabulary\n",
    "for sentence, _ in chinese_sentences:\n",
    "    for token in nlp_zh(sentence):\n",
    "        if token.text not in TEXT.vocab.stoi:\n",
    "            TEXT.vocab.stoi[token.text] = len(TEXT.vocab.stoi)\n",
    "            TEXT.vocab.itos.append(token.text)\n"
   ],
   "id": "4fd7fbfd3a97a310",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create Iterator",
   "id": "765527cba4770ddc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T02:42:48.607837Z",
     "start_time": "2024-07-09T02:42:48.603398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create iterators\n",
    "BATCH_SIZE = 4\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=torch.device('mps')\n",
    ")\n",
    "\n",
    "chinese_iterator = data.BucketIterator(\n",
    "    chinese_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=torch.device('mps')\n",
    ")"
   ],
   "id": "cbd3b2a51f3d7910",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create the Model\n",
    "\n",
    "Because the situation is quite easy, we can only define a layer of `LSTM` instead of using `transformers` to convert the text into a vector representation. The `LSTM` layer is used to convert the text into a vector representation. The vector representation can be used as input to the neural network.\n",
    "\n",
    "The `LSTM` layer is a type of recurrent neural network that can be used to process the sequence data. The `LSTM` layer is used to process the text and convert it into a vector representation. The vector representation can be used as input to the neural network."
   ],
   "id": "f2b6851d96f5057a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T02:42:50.582881Z",
     "start_time": "2024-07-09T02:42:50.539176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        return self.fc(hidden)\n",
    "\n",
    "# Define hyperparameters\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ],
   "id": "a2798fc0cc1887c8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train the Model",
   "id": "6f49a3d88ea41e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T02:43:03.467360Z",
     "start_time": "2024-07-09T02:43:03.447400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('mps')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    return correct.sum() / len(correct)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in tqdm(iterator):\n",
    "        text = batch.text.to(device)\n",
    "        label = batch.label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text).squeeze(1)\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = binary_accuracy(predictions, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iterator):\n",
    "            text = batch.text.to(device)\n",
    "            label = batch.label.to(device)\n",
    "            predictions = model(text).squeeze(1)\n",
    "            loss = criterion(predictions, label)\n",
    "            acc = binary_accuracy(predictions, label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "# N_EPOCHS = 5\n",
    "\n",
    "# for epoch in range(N_EPOCHS):\n",
    "#     train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "#     valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "#     print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "# Load the state dict\n",
    "model.load_state_dict(torch.load('./data/03-state_dict.pth'))"
   ],
   "id": "9a525aef37885fea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test the Model",
   "id": "898989102647a1e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T02:45:09.917572Z",
     "start_time": "2024-07-09T02:43:06.876978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
   ],
   "id": "bd94ec01c2c5d822",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 6054/6250 [02:00<00:03, 50.08it/s] \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 120.00 MB, other allocations: 18.01 GB, max allowed: 18.13 GB). Tried to allocate 6.22 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m test_loss, test_acc \u001B[38;5;241m=\u001B[39m evaluate(model, test_iterator, criterion)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTest Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Test Acc: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_acc\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[8], line 45\u001B[0m, in \u001B[0;36mevaluate\u001B[0;34m(model, iterator, criterion)\u001B[0m\n\u001B[1;32m     43\u001B[0m text \u001B[38;5;241m=\u001B[39m batch\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     44\u001B[0m label \u001B[38;5;241m=\u001B[39m batch\u001B[38;5;241m.\u001B[39mlabel\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 45\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model(text)\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     46\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(predictions, label)\n\u001B[1;32m     47\u001B[0m acc \u001B[38;5;241m=\u001B[39m binary_accuracy(predictions, label)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/training/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/training/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[5], line 13\u001B[0m, in \u001B[0;36mSentimentRNN.forward\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, text):\n\u001B[1;32m     12\u001B[0m     embedded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(text))\n\u001B[0;32m---> 13\u001B[0m     output, (hidden, cell) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrnn(embedded)\n\u001B[1;32m     14\u001B[0m     hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(torch\u001B[38;5;241m.\u001B[39mcat((hidden[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m,:,:], hidden[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,:,:]), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(hidden)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/training/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/training/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/training/lib/python3.11/site-packages/torch/nn/modules/rnn.py:911\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[0;34m(self, input, hx)\u001B[0m\n\u001B[1;32m    908\u001B[0m         hx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpermute_hidden(hx, sorted_indices)\n\u001B[1;32m    910\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 911\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\u001B[38;5;28minput\u001B[39m, hx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers,\n\u001B[1;32m    912\u001B[0m                       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first)\n\u001B[1;32m    913\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    914\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\u001B[38;5;28minput\u001B[39m, batch_sizes, hx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias,\n\u001B[1;32m    915\u001B[0m                       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: MPS backend out of memory (MPS allocated: 120.00 MB, other allocations: 18.01 GB, max allowed: 18.13 GB). Tried to allocate 6.22 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Predict the Sentiment",
   "id": "87e7e6b42806e070"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T02:45:41.860387Z",
     "start_time": "2024-07-09T02:45:41.837105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_sentiment(model, sentence, nlp, min_len=5):\n",
    "    model.eval()\n",
    "    tokens = [tok.text for tok in nlp(sentence)]\n",
    "    if len(tokens) < min_len:\n",
    "        tokens += ['<pad>'] * (min_len - len(tokens))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokens]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    if prediction.item() >= 0.5:\n",
    "        return 'pos'\n",
    "    else:\n",
    "        return 'neg'\n",
    "\n",
    "# English sentence\n",
    "print(predict_sentiment(model, \"This movie so great!\", nlp_en))\n",
    "# Chinese sentence\n",
    "print(predict_sentiment(model, \"我不高兴……\", nlp_en))"
   ],
   "id": "e5a1d4b428eb4242",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "neg\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save the Model",
   "id": "d85416dbf74e0f17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.save(model.state_dict(), './data/03-state_dict.pth')\n",
    "torch.save(model, './data/03-model.pth')"
   ],
   "id": "1bc5baf85a262520",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Play with the Model",
   "id": "b8d78119472dc25a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T02:46:03.074052Z",
     "start_time": "2024-07-09T02:45:47.471125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "while True:\n",
    "    sentence = input(\"Enter a sentence: \")\n",
    "    if sentence == 'exit':\n",
    "        break\n",
    "    print(predict_sentiment(model, sentence, nlp_en))"
   ],
   "id": "9661d249add4cab9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "pos\n",
      "pos\n",
      "neg\n",
      "neg\n",
      "neg\n",
      "neg\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "69a7fb8fd47a6a60"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
