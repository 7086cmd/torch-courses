{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Course 3: Embedded Layer for Natural Language Processing\n",
    "\n",
    "In this notebook, we will use `PyTorch` to get started with natural language processing. I recommend you to use `spaCy` to do the tokenization and the preprocessing of the text. It is more powerful than the `nltk` library."
   ],
   "id": "8016e6566ef10024"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Install the libraries\n",
    "\n",
    "The `spaCy` library is not installed by default. You can install it by running the following command:"
   ],
   "id": "165ecc5167420199"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T05:25:53.550438Z",
     "start_time": "2024-07-07T05:25:53.548367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if using macOS\n",
    "# !pip install 'spacy[transformers,lookups,apple]' torchtext transformers\n",
    "# if using Windows with GPU\n",
    "# !pip install -U 'spacy[cuda11x,transformers,lookups]' torchtext transformers\n",
    "# if using Windows without GPU\n",
    "# !pip install -U 'spacy[transformers,lookups]' torchtext transformers\n",
    "# !pip install spacy torchtext transformers\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download zh_core_web_sm"
   ],
   "id": "57487e9053db0b0a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Because the competition contains Chinese text, we will use the `zh_core_web_sm` model to tokenize the Chinese text. The `en_core_web_sm` model is used to tokenize the English text.\n",
    "\n",
    "The `torchtext` library is used to preprocess the text and create the dataloader. It is a powerful library that can help you to preprocess the text and create the dataloader in a few lines of code.\n",
    "\n",
    "The `transformers` library is used to load the pre-trained model, provided by the Hugging Face team. The pre-trained model is used to convert the text into a vector representation. The vector representation can be used as input to the neural network.\n",
    "\n",
    "However, it is not recommended to use pre-trained model during the competition. But it is a good practice to use it in the real world project.\n",
    "\n",
    "Let's start by importing the libraries."
   ],
   "id": "ffa7dea61c84c876"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T05:26:18.821690Z",
     "start_time": "2024-07-07T05:25:53.558801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torchtext import data, datasets\n",
    "import spacy\n",
    "\n",
    "# Load spaCy models\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_zh = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "# Define the fields\n",
    "TEXT = data.Field(tokenize=lambda x: [tok.text for tok in nlp_en.tokenizer(x)], lower=True)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "# Load IMDb dataset\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train_data, valid_data = train_data.split()"
   ],
   "id": "39eea7c1902c15ce",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The `TEXT` field is used to tokenize the text. The `LABEL` field is used to store the label. The `datasets.IMDB.splits` function is used to load the IMDb dataset. The `train_data` and `test_data` are used to store the training and testing data. The `train_data` is further split into `train_data` and `valid_data`.",
   "id": "ba4c7c136f8eaff7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Transplant to Chinese Custom Dataset",
   "id": "bfe4ab9b10d3c7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T05:26:19.205768Z",
     "start_time": "2024-07-07T05:26:18.822902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Custom dataset for Chinese (example)\n",
    "chinese_sentences = [\n",
    "    (\"我今天很开心\", \"pos\"),\n",
    "    (\"这真是糟糕透了\", \"neg\"),\n",
    "    (\"天气不错\", \"pos\"),\n",
    "    (\"我感到很悲伤\", \"neg\")\n",
    "]\n",
    "\n",
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self, examples, fields, **kwargs):\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, fields, train_examples=None, val_examples=None, test_examples=None, **kwargs):\n",
    "        return (cls(train_examples, fields, **kwargs),\n",
    "                cls(val_examples, fields, **kwargs),\n",
    "                cls(test_examples, fields, **kwargs))\n",
    "\n",
    "# Create examples for the custom dataset\n",
    "chinese_examples = [data.Example.fromlist([sentence, label], [('text', TEXT), ('label', LABEL)]) for sentence, label in chinese_sentences]\n",
    "chinese_dataset = CustomDataset(chinese_examples, [('text', TEXT), ('label', LABEL)])\n",
    "\n",
    "# Build the vocabulary\n",
    "TEXT.build_vocab(train_data, max_size=25000)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# Add Chinese tokens to the vocabulary\n",
    "for sentence, _ in chinese_sentences:\n",
    "    for token in nlp_zh(sentence):\n",
    "        if token.text not in TEXT.vocab.stoi:\n",
    "            TEXT.vocab.stoi[token.text] = len(TEXT.vocab.stoi)\n",
    "            TEXT.vocab.itos.append(token.text)\n"
   ],
   "id": "4fd7fbfd3a97a310",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create Iterator",
   "id": "765527cba4770ddc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T05:26:19.208342Z",
     "start_time": "2024-07-07T05:26:19.206471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create iterators\n",
    "BATCH_SIZE = 16\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=torch.device('mps')\n",
    ")\n",
    "\n",
    "chinese_iterator = data.BucketIterator(\n",
    "    chinese_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=torch.device('mps')\n",
    ")"
   ],
   "id": "cbd3b2a51f3d7910",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create the Model\n",
    "\n",
    "Because the situation is quite easy, we can only define a layer of `LSTM` instead of using `transformers` to convert the text into a vector representation. The `LSTM` layer is used to convert the text into a vector representation. The vector representation can be used as input to the neural network.\n",
    "\n",
    "The `LSTM` layer is a type of recurrent neural network that can be used to process the sequence data. The `LSTM` layer is used to process the text and convert it into a vector representation. The vector representation can be used as input to the neural network."
   ],
   "id": "f2b6851d96f5057a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T05:26:19.243118Z",
     "start_time": "2024-07-07T05:26:19.209453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        return self.fc(hidden)\n",
    "\n",
    "# Define hyperparameters\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ],
   "id": "a2798fc0cc1887c8",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train the Model",
   "id": "6f49a3d88ea41e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T05:49:59.493029Z",
     "start_time": "2024-07-07T05:28:04.960954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('mps')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    return correct.sum() / len(correct)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in tqdm(iterator):\n",
    "        text = batch.text.to(device)\n",
    "        label = batch.label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text).squeeze(1)\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = binary_accuracy(predictions, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iterator):\n",
    "            text = batch.text.to(device)\n",
    "            label = batch.label.to(device)\n",
    "            predictions = model(text).squeeze(1)\n",
    "            loss = criterion(predictions, label)\n",
    "            acc = binary_accuracy(predictions, label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')"
   ],
   "id": "9a525aef37885fea",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1094/1094 [04:18<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.684, Train Acc: 55.74%, Val. Loss: 0.685, Val. Acc: 54.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1094/1094 [04:08<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train Loss: 0.671, Train Acc: 57.58%, Val. Loss: 0.660, Val. Acc: 62.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1094/1094 [04:07<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Train Loss: 0.596, Train Acc: 68.49%, Val. Loss: 0.512, Val. Acc: 75.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1094/1094 [04:05<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Train Loss: 0.483, Train Acc: 77.14%, Val. Loss: 0.389, Val. Acc: 83.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1094/1094 [04:04<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Train Loss: 0.421, Train Acc: 81.44%, Val. Loss: 0.346, Val. Acc: 85.32%\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test the Model",
   "id": "898989102647a1e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T05:53:06.527435Z",
     "start_time": "2024-07-07T05:52:24.453180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
   ],
   "id": "bd94ec01c2c5d822",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.370, Test Acc: 84.15%\n",
      "pos\n",
      "neg\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Predict the Sentiment",
   "id": "87e7e6b42806e070"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T05:55:20.034625Z",
     "start_time": "2024-07-07T05:55:19.985473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_sentiment(model, sentence, nlp, min_len=5):\n",
    "    model.eval()\n",
    "    tokens = [tok.text for tok in nlp(sentence)]\n",
    "    if len(tokens) < min_len:\n",
    "        tokens += ['<pad>'] * (min_len - len(tokens))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokens]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    if prediction.item() >= 0.5:\n",
    "        return 'pos'\n",
    "    else:\n",
    "        return 'neg'\n",
    "\n",
    "# English sentence\n",
    "print(predict_sentiment(model, \"This movie is great!\", nlp_en))\n",
    "# Chinese sentence\n",
    "print(predict_sentiment(model, \"我不高兴……\", nlp_zh))"
   ],
   "id": "e5a1d4b428eb4242",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n",
      "neg\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save the Model",
   "id": "d85416dbf74e0f17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T05:54:11.334404Z",
     "start_time": "2024-07-07T05:54:11.270983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.save(model.state_dict(), './data/03-state_dict.pth')\n",
    "torch.save(model, './data/03-model.pth')"
   ],
   "id": "1bc5baf85a262520",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Play with the Model",
   "id": "b8d78119472dc25a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T05:57:25.819098Z",
     "start_time": "2024-07-07T05:57:12.488918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "while True:\n",
    "    sentence = input(\"Enter a sentence: \")\n",
    "    if sentence == 'exit':\n",
    "        break\n",
    "    print(predict_sentiment(model, sentence, nlp_en))"
   ],
   "id": "9661d249add4cab9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n",
      "pos\n",
      "neg\n"
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
