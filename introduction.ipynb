{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Course 1: Introduction to PyTorch\n",
    "\n",
    "PyTorch is a great framework for machine learning. It is a python package that provides two high-level features:\n",
    "\n",
    "1. Tensor computation (like numpy) with strong GPU acceleration\n",
    "2. Deep Neural Networks built on a tape-based autograd system\n",
    "\n",
    "You can reuse your favorite python packages such as numpy, scipy, etc. using PyTorch. PyTorch also provides a module named `torch.nn` that provides a higher level of abstraction over raw computational graphs created by using `torch.autograd`.\n",
    "\n",
    "In this course, we will cover the basics of PyTorch and build a simple neural network to classify handwritten digits through MNIST dataset."
   ],
   "id": "1fdeae0bf262563f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Installation\n",
    "\n",
    "You can install PyTorch using pip. You can find the installation instructions on the official website of PyTorch. Here is the link to the installation page: [PyTorch Installation](https://pytorch.org/get-started/locally/). However, it is recommended to create a virtual environment before installing PyTorch via `conda`, and install the required packages.\n",
    "\n",
    "### Installation of Anaconda\n",
    "\n",
    "You can download Anaconda from the following link: [Anaconda Download](https://www.anaconda.com/products/distribution). After downloading, you can install Anaconda by following the instructions provided on the official website.\n",
    "\n",
    "It is worthwhile to figure out that it is not recommended to install packages directly on `base` environment. Instead, you can create a new environment and install the required packages in that environment:\n"
   ],
   "id": "7df827aeb6469745"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!conda create --name training python=3.11\n",
    "!conda activate training"
   ],
   "id": "42ceff980b24cbb2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Installation of PyTorch\n",
    "\n",
    "If you have Nvidia GPU, you can install PyTorch with GPU support. Otherwise, you can install PyTorch without GPU support. (Apple MPS is supported natively with `CPU` package, but CUDA is not directly included but requires extra installation)."
   ],
   "id": "1f31e0760c037c50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PyTorch Basics\n",
    "\n",
    "The basic building block of PyTorch is a tensor. A tensor is a number, vector, matrix, or any n-dimensional array. You can create a tensor using the `torch` module. Here is an example of creating a tensor:"
   ],
   "id": "46a721f8ec1b612"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:43:40.690016Z",
     "start_time": "2024-07-05T07:43:39.749728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "x = torch.tensor([5.5, 3])\n",
    "\n",
    "print(x)\n",
    "\n",
    "# Get the size of the tensor\n",
    "print(x.size())"
   ],
   "id": "259bedcdfa273860",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Operations on Tensors\n",
    "\n",
    "You can perform operations on tensors. Here is an example of performing operations on tensors:"
   ],
   "id": "e93d5c737b4d4f1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:44:23.937448Z",
     "start_time": "2024-07-05T07:44:23.913319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a tensor\n",
    "x = torch.tensor([5.5, 3])\n",
    "\n",
    "# Create another tensor\n",
    "y = torch.tensor([2, 1])\n",
    "\n",
    "# Add two tensors\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "# Multiply two tensors\n",
    "z = x * y\n",
    "print(z)\n",
    "\n",
    "# Multiply a tensor by a scalar\n",
    "z = x * 2\n",
    "print(z)"
   ],
   "id": "2f603d828c568270",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.5000, 4.0000])\n",
      "tensor([11.,  3.])\n",
      "tensor([11.,  6.])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Broadcasting\n",
    "\n",
    "Broadcasting is a powerful mechanism that allows PyTorch to work with tensors of different shapes. Here is an example of broadcasting:"
   ],
   "id": "7f7a1f81439e44be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:45:00.324949Z",
     "start_time": "2024-07-05T07:45:00.318671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a tensor\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Create another tensor\n",
    "y = torch.tensor([1, 0, -1])\n",
    "\n",
    "# Add two tensors\n",
    "z = x + y\n",
    "print(z)"
   ],
   "id": "414a188749ab6af9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 2, 2],\n",
      "        [5, 5, 5],\n",
      "        [8, 8, 8]])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It is clear that when adding `y` to `x`, `y` is broadcasted to the shape of `x`. This is a powerful mechanism that allows PyTorch to work with tensors of different shapes.",
   "id": "5323ce7a10c0e73d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Autograd: Automatic Differentiation\n",
    "\n",
    "Central to all neural networks in PyTorch is the `autograd` package. The `autograd` package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backpropagation is defined by how your code is run, and that every single iteration can be different.\n",
    "\n",
    "### Tensor\n",
    "\n",
    "`torch.Tensor` is the central class of the package. If you set its attribute `.requires_grad` as `True`, it starts to track all operations on it. When you finish your computation, you can call `.backward()` and have all the gradients computed automatically. The gradient for this tensor will be accumulated into `.grad` attribute."
   ],
   "id": "30da9174c1b565fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:46:48.423630Z",
     "start_time": "2024-07-05T07:46:48.374879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a tensor\n",
    "x = torch.tensor([5.5, 3], requires_grad=True)\n",
    "\n",
    "# Perform operations on the tensor\n",
    "y = x + 2\n",
    "\n",
    "# Perform more operations on the tensor\n",
    "z = y * y * 2\n",
    "\n",
    "# Compute the mean of the tensor\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)\n",
    "\n",
    "# Compute the gradients\n",
    "out.backward()\n",
    "\n",
    "# Print the gradients\n",
    "print(x.grad)"
   ],
   "id": "988a1e9118bf8c50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([112.5000,  50.0000], grad_fn=<MulBackward0>) tensor(81.2500, grad_fn=<MeanBackward0>)\n",
      "tensor([15., 10.])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Vector-Jacobian Product\n",
    "\n",
    "In the previous example, we computed the gradients of the tensor. However, what if we have a vector-Jacobian product? In PyTorch, you can compute the vector-Jacobian product using the `backward` function. Here is an example of computing the vector-Jacobian product:"
   ],
   "id": "97a7142d6b003710"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:47:36.322878Z",
     "start_time": "2024-07-05T07:47:35.990335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a tensor\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "# Perform operations on the tensor\n",
    "y = x * 2\n",
    "\n",
    "# Compute the norm of the tensor\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "# Create a vector\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "\n",
    "# Compute the vector-Jacobian product\n",
    "y.backward(v)\n",
    "\n",
    "# Print the gradients\n",
    "print(x.grad)"
   ],
   "id": "90879020f3c469f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Stop Autograd\n",
    "\n",
    "By default, all tensors have `requires_grad=True`. However, if you want to stop autograd, you can use the `torch.no_grad()` block. Here is an example of stopping autograd:"
   ],
   "id": "9b43d513f1702e1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:49:07.178380Z",
     "start_time": "2024-07-05T07:49:07.170370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a tensor\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "# Perform operations on the tensor\n",
    "print(x.requires_grad)\n",
    "\n",
    "# Stop autograd\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ],
   "id": "7e12a1c25f40573d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Neural Networks\n",
    "\n",
    "Neural networks can be constructed using the `torch.nn` package. An `nn.Module` contains layers, and a method `forward(input)` that returns the `output`, and there is a `functional` subpackage that contains many useful functions like `ReLU`, `Sigmoid`, etc.\n",
    "\n",
    "Since I am using Apple MacBook Pro with M2 Pro chip, I will use `mps` as the device. You can use `cuda` if you have an Nvidia GPU.\n",
    "\n",
    "Here is an example of constructing a neural network:"
   ],
   "id": "f88d167695649223"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:53:43.648327Z",
     "start_time": "2024-07-05T07:53:43.638186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net().to(device)\n",
    "\n",
    "print(net)"
   ],
   "id": "ba89c1cde7a7db98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The `forward` function defines the computation performed at every call. You can use any tensor operation in the `forward` function.\n",
    "\n",
    "The learnable parameters of a model are returned by `net.parameters()`. Here is an example of getting the learnable parameters of the model:"
   ],
   "id": "b6d01cf93fbd150f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:53:48.105151Z",
     "start_time": "2024-07-05T07:53:48.101978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ],
   "id": "636273b912ab6579",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loss Function\n",
    "\n",
    "A loss function takes the `(output, target)` pair of inputs and computes a value that estimates how far away the output is from the target. There are several different loss functions under the `torch.nn` package. Here is an example of computing the loss function:"
   ],
   "id": "64c66f79242d718d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:53:50.996985Z",
     "start_time": "2024-07-05T07:53:50.008995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input = torch.randn(1, 1, 32, 32).to(device)\n",
    "output = net(input)\n",
    "target = torch.randn(10).to(device)\n",
    "target = target.view(1, -1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ],
   "id": "6adc68aad6f32013",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2022, device='mps:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Backpropagation\n",
    "\n",
    "To backpropagate the error, we need to call the `loss.backward()`. However, before calling `loss.backward()`, the gradients of the model should be zeroed. Here is an example of backpropagation:"
   ],
   "id": "7a76d8224898c1e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "net.zero_grad()",
   "id": "f39c4116c10edb41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Optimizing the Neural Network\n",
    "\n",
    "The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):"
   ],
   "id": "d726363cb1b4f29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)"
   ],
   "id": "91d125b7b221d304"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here is an example of optimizing the neural network:\n",
   "id": "9cb6f650f892036a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "optimizer.zero_grad()\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ],
   "id": "ec6ef28877b42aa2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training a Neural Network\n",
    "\n",
    "Training a neural network typically consists of the following steps:\n",
    "\n",
    "1. Load the data into `DataLoader`, especially transform the data into a tensor.\n",
    "2. Define the neural network model.\n",
    "3. Define the loss function.\n",
    "4. Define the optimizer.\n",
    "5. Train the neural network.\n",
    "6. Evaluate the neural network.\n",
    "\n",
    "Before training the network via MNIST, let us get started with `torchvision` package."
   ],
   "id": "16ae8694c3f2a971"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d6c9e0033491cac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
